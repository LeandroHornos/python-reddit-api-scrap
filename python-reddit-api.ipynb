{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f12e88",
   "metadata": {},
   "source": [
    "# Reddit API scraping with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd0befaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libs\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Connection\n",
    "import requests\n",
    "\n",
    "# Data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b09ab4",
   "metadata": {},
   "source": [
    "### Connecting to Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563237a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHeaders():\n",
    "    with open(\"secret.json\") as file:\n",
    "        client_id, secret, password = json.load(file).values()\n",
    "    \n",
    "    user_data = {\n",
    "        \"grant_type\":\"password\",\n",
    "        \"username\":\"Hungry_Fuel_2913\",\n",
    "        \"password\":password\n",
    "    }\n",
    "    \n",
    "    auth = requests.auth.HTTPBasicAuth(client_id, secret)\n",
    "    headers = {\"User-Agent\":\"Python-API/0.0.1\"}\n",
    "    res = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=auth, data=user_data, headers = headers)\n",
    "    TOKEN = res.json()[\"access_token\"]\n",
    "    headers[\"Authorization\"] = f\"bearer {TOKEN}\"\n",
    "    \n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4692c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = getHeaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba8e1c",
   "metadata": {},
   "source": [
    "### Getting my User Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49920bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_info = requests.get(\"https://oauth.reddit.com/api/v1/me\", headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a34e302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['is_employee', 'seen_layout_switch', 'has_visited_new_profile', 'pref_no_profanity', 'has_external_account', 'pref_geopopular', 'seen_redesign_modal', 'pref_show_trending', 'subreddit', 'pref_show_presence', 'snoovatar_img', 'snoovatar_size', 'gold_expiration', 'has_gold_subscription', 'is_sponsor', 'num_friends', 'features', 'can_edit_name', 'verified', 'new_modmail_exists', 'pref_autoplay', 'coins', 'has_paypal_subscription', 'has_subscribed_to_premium', 'id', 'has_stripe_subscription', 'oauth_client_id', 'can_create_subreddit', 'over_18', 'is_gold', 'is_mod', 'awarder_karma', 'suspension_expiration_utc', 'has_verified_email', 'is_suspended', 'pref_video_autoplay', 'has_android_subscription', 'in_redesign_beta', 'icon_img', 'has_mod_mail', 'pref_nightmode', 'awardee_karma', 'hide_from_robots', 'password_set', 'link_karma', 'force_password_reset', 'total_karma', 'seen_give_award_tooltip', 'inbox_count', 'seen_premium_adblock_modal', 'pref_top_karma_subreddits', 'has_mail', 'pref_show_snoovatar', 'name', 'pref_clickgadget', 'created', 'gold_creddits', 'created_utc', 'has_ios_subscription', 'pref_show_twitter', 'in_beta', 'comment_karma', 'accept_followers', 'has_subscribed', 'linked_identities', 'seen_subreddit_chat_ftux'])\n"
     ]
    }
   ],
   "source": [
    "print(my_info.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f093a65",
   "metadata": {},
   "source": [
    "## Scrapping Reddit Argentina"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccba8a3",
   "metadata": {},
   "source": [
    "### Hot topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b290439",
   "metadata": {},
   "source": [
    "If we use the endpoint /subreddit/hot we get the hottest topics of that subreddit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d275e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = getHeaders()\n",
    "res = requests.get(\"https://oauth.reddit.com/r/argentina/hot\", headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "530cdc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPostInfo(keys_to_extract=[], postList=[]):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for post in postList:\n",
    "        data = post[\"data\"]\n",
    "        row = {}\n",
    "        row[\"kind\"] = post[\"kind\"]\n",
    "        for key in keys_to_extract:\n",
    "            row[key] = data[key]\n",
    "        df = df.append(row, ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69b494e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kind': 't3', 'data': {'approved_at_utc': None, 'subreddit': 'argentina', 'selftext': 'Thread Diario de Dudas y Consultas!\\n\\nEntra a nuestro [Discord](https://discord.gg/PeaSPMPn46) y charla con la comunidad!', 'author_fullname': 't2_f0mqjq9a', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Thread Diario de Dudas, Consultas y Mitaps - 23/03', 'link_flair_richtext': [{'e': 'text', 't': 'Sticky 游늷'}], 'subreddit_name_prefixed': 'r/argentina', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'sticky', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_tkpp33', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 17, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Sticky 游늷', 'can_mod_post': False, 'score': 17, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1648026001.0, 'link_flair_type': 'richtext', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.argentina', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thread Diario de Dudas y Consultas!&lt;/p&gt;\\n\\n&lt;p&gt;Entra a nuestro &lt;a href=\"https://discord.gg/PeaSPMPn46\"&gt;Discord&lt;/a&gt; y charla con la comunidad!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': 'new', 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': True, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/8Zq-3MWu94TqE2KLqM1CpmdL6pHiX5T3wtqCQWsZenA.jpg?auto=webp&amp;s=c574eb8108d4d9ccd0d1d58ee2085e859bd06149', 'width': 256, 'height': 256}, 'resolutions': [{'url': 'https://external-preview.redd.it/8Zq-3MWu94TqE2KLqM1CpmdL6pHiX5T3wtqCQWsZenA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9e35d39662c2d7f85b900d9a1192fea9164677f7', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/8Zq-3MWu94TqE2KLqM1CpmdL6pHiX5T3wtqCQWsZenA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=90e2a9b67fe9d1e60e05e359da5b08dc0125d79d', 'width': 216, 'height': 216}], 'variants': {}, 'id': '1THVFoRr0pHr9pKnSlH0Bb-JVjmHgBgzT5-HsH4I_Nw'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'bf2e47cc-f7e5-11e2-9902-12313b0cf20e', 'can_gild': True, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qlht', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#94e044', 'id': 'tkpp33', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Robo-TINA', 'discussion_type': None, 'num_comments': 1008, 'send_replies': False, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/argentina/comments/tkpp33/thread_diario_de_dudas_consultas_y_mitaps_2303/', 'parent_whitelist_status': 'all_ads', 'stickied': True, 'url': 'https://www.reddit.com/r/argentina/comments/tkpp33/thread_diario_de_dudas_consultas_y_mitaps_2303/', 'subreddit_subscribers': 380822, 'created_utc': 1648026001.0, 'num_crossposts': 0, 'media': None, 'is_video': False}}\n"
     ]
    }
   ],
   "source": [
    "print(res.json()[\"data\"][\"children\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb8cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_extract = [\"subreddit\", \"title\", \"author\", \n",
    "                   \"permalink\", \"url\", \"num_comments\", \"ups\", \"selftext\", \"id\",\n",
    "                   \"downs\", \"upvote_ratio\", \"score\",\"created\", \"edited\", \"author_fullname\", \"subreddit_id\" ]\n",
    "\n",
    "df = extractPostInfo(keys_to_extract, res.json()[\"data\"][\"children\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e51e55e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Miercoles de Rant</td>\n",
       "      <td>0.81</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Se puede</td>\n",
       "      <td>0.84</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hace un tiempo me regalaron una tablet de dibu...</td>\n",
       "      <td>0.84</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Javier Milei asegur칩 que la soluci칩n contra la...</td>\n",
       "      <td>0.87</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"Por favor no me pese\": la campa침a para evitar...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  upvote_ratio    ups  \\\n",
       "1                                   Miercoles de Rant          0.81   13.0   \n",
       "21                                           Se puede          0.84   59.0   \n",
       "7   Hace un tiempo me regalaron una tablet de dibu...          0.84  138.0   \n",
       "26  Javier Milei asegur칩 que la soluci칩n contra la...          0.87  127.0   \n",
       "18  \"Por favor no me pese\": la campa침a para evitar...          0.90   87.0   \n",
       "\n",
       "    downs  score  \n",
       "1     0.0   13.0  \n",
       "21    0.0   59.0  \n",
       "7     0.0  138.0  \n",
       "26    0.0  127.0  \n",
       "18    0.0   87.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values([\"upvote_ratio\",\"score\"], ascending=True)[[\"title\",\"upvote_ratio\",\"ups\", \"downs\", \"score\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d20ee5",
   "metadata": {},
   "source": [
    "It is interesting to note that, even though we observe many upvote ratios values under 1, the number of downs is always zero. Maybe it is a bug, or maybe Reddit doesn't want us to know the number of downvotes a post has. Whatever the case, we can infer the number of downvotes from the number of upvotes an the ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02056437",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def guessDownsFromRatio(ups, ratio):\n",
    "    ups = float(ups)\n",
    "    ratio = float(ratio)\n",
    "    if ratio > 0:\n",
    "        return math.floor(ups*( (1-ratio) / ratio  ) )\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "df[\"guessed_downs\"] = df.apply(lambda row: guessDownsFromRatio(row[\"ups\"], row[\"upvote_ratio\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc56c077",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>ups</th>\n",
       "      <th>guessed_downs</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Miercoles de Rant</td>\n",
       "      <td>0.81</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Se puede</td>\n",
       "      <td>0.84</td>\n",
       "      <td>59.0</td>\n",
       "      <td>11</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hace un tiempo me regalaron una tablet de dibu...</td>\n",
       "      <td>0.84</td>\n",
       "      <td>138.0</td>\n",
       "      <td>26</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Javier Milei asegur칩 que la soluci칩n contra la...</td>\n",
       "      <td>0.87</td>\n",
       "      <td>127.0</td>\n",
       "      <td>18</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"Por favor no me pese\": la campa침a para evitar...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>87.0</td>\n",
       "      <td>9</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  upvote_ratio    ups  \\\n",
       "1                                   Miercoles de Rant          0.81   13.0   \n",
       "21                                           Se puede          0.84   59.0   \n",
       "7   Hace un tiempo me regalaron una tablet de dibu...          0.84  138.0   \n",
       "26  Javier Milei asegur칩 que la soluci칩n contra la...          0.87  127.0   \n",
       "18  \"Por favor no me pese\": la campa침a para evitar...          0.90   87.0   \n",
       "\n",
       "    guessed_downs  score  \n",
       "1               3   13.0  \n",
       "21             11   59.0  \n",
       "7              26  138.0  \n",
       "26             18  127.0  \n",
       "18              9   87.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values([\"upvote_ratio\",\"score\"], ascending=True)[[\"title\",\"upvote_ratio\",\"ups\", \"guessed_downs\", \"score\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42d7d391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/r/argentina/comments/tl4c8k/por_favor_no_me_pese_la_campa침a_para_evitar_que/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[18][\"permalink\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0d5f9",
   "metadata": {},
   "source": [
    "## All Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8306e700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllPosts(keys_to_extract, headers, subreddit=\"argentina\", paused=True):\n",
    "    '''\n",
    "    This function accesses a subredit and then iteratively retrieves the last\n",
    "    100 posts until there are no more posts to be accessed, then extracts the\n",
    "    desired data from the json responses and returns it as a pandas dataframe.\n",
    "    \n",
    "    This function recieves a list of the keys of the json response that are \n",
    "    to be extracted    as an argument. The headers allow the function to get \n",
    "    access to the reddit api.\n",
    "    \n",
    "    The default subreddit is /argentina since this function was originaly created\n",
    "    to scrap that subreddit. Another subreddit can be passed as an argument.\n",
    "    The script takes a pause of 5 seconds between requests to avoid making too much\n",
    "    calls to the api. This can be overriden by pasing the pause=False argument.\n",
    "    \n",
    "\n",
    "    '''\n",
    "\n",
    "    print(f\"Obtaining data from Reddit/{subreddit}...\")\n",
    "    \n",
    "    # Create an empty dataframe and get the first group of posts\n",
    "    df = pd.DataFrame()\n",
    "    res = requests.get(f\"https://oauth.reddit.com/r/{subreddit}/new\", headers=headers, params={\"limit\":\"100\"})\n",
    "    # If something went wrong, stop.\n",
    "    if(res.status_code != 200):\n",
    "        print(\"Data cannot be accessed\", f\"Status code: {res.status_code}\")\n",
    "        return df\n",
    "\n",
    "    # Keep getting older posts\n",
    "    while (res.status_code == 200):\n",
    "        # Extact current data and store it in the dataframe\n",
    "        df_current = extractPostInfo(keys_to_extract, res.json()[\"data\"][\"children\"])\n",
    "        if df_current.shape[0] == 0:\n",
    "            break\n",
    "        df = pd.concat([df, df_current])\n",
    "        # Get the id of the last obtained post to know where to start next\n",
    "        last = df.iloc[-1][\"kind\"] + \"_\" + df.iloc[-1][\"id\"]\n",
    "        print(\"last:\", last)\n",
    "        # This delay is there just in case, we don't want to get banned\n",
    "        if paused:\n",
    "            time.sleep(5) \n",
    "        # Show some feedback on screen to let the user know what is going on\n",
    "        print(f\"obtained {df_current.shape[0]} posts\")\n",
    "        print(f\"total: {df.shape[0]} posts\")\n",
    "        print(\"next!\")\n",
    "        # Try to get the next group of posts and restart the loop\n",
    "        res = requests.get(f\"https://oauth.reddit.com/r/{subreddit}/new\", headers=headers, params={\"limit\":\"100\", \"after\":last})\n",
    "        print(\"Status code\", res.status_code)\n",
    "        \n",
    "    print(\"done!\")\n",
    "    # We now guess the number of downvotes:\n",
    "    df[\"guessed_downs\"] = df.apply(lambda row: guessDownsFromRatio(row[\"ups\"], row[\"upvote_ratio\"]), axis=1)\n",
    "    # Return the data\n",
    "    return df      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd931c1",
   "metadata": {},
   "source": [
    "## Expand and unify data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f2c4da",
   "metadata": {},
   "source": [
    "Previously we have extracted all the available posts in a particular subreddit, and stored that as a csv, named with the timestamp at the moment of the extraction. The information that a particular post contains changes over time. New versions of the csv will contain both new posts and newer version of some of the posts in previous csv. \n",
    "<br>\n",
    "What we need to do is:\n",
    "<ol>\n",
    "    <li>Load the current dataframe and the previous csv </li>\n",
    "    <li>New posts are added to the old dataset</li>\n",
    "    <li>Existing posts are updated with the newest data</li>\n",
    "    <li>Posts not present in current dataframe are left unchanged</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b523f020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineOnColAndUpdate(old_df, new_df, index_col):\n",
    "    '''\n",
    "    Takes a dataframe with the existing data and\n",
    "    updates the threads that exist in both dataframes\n",
    "    with values from the latest version.\n",
    "    It also adds the new threads that were not present in\n",
    "    the old dataframe.\n",
    "    Threads are identified by their fullname, wich is\n",
    "    composed of the kind and the id separated by a lodash\n",
    "\n",
    "    '''\n",
    "    a = new_df.set_index(index_col)\n",
    "    b = old_df.set_index(index_col)\n",
    "    df = (a.combine_first(b)).reset_index().reindex(columns=old_df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6beda8",
   "metadata": {},
   "source": [
    "### Getting all post and updating our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b99797d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining data from Reddit/argentina...\n",
      "last: t3_tkdphm\n",
      "obtained 100 posts\n",
      "total: 100 posts\n",
      "next!\n",
      "Status code 200\n",
      "last: t3_tjdvei\n",
      "obtained 100 posts\n",
      "total: 200 posts\n",
      "next!\n",
      "Status code 200\n",
      "last: t3_ti2uby\n",
      "obtained 100 posts\n",
      "total: 300 posts\n",
      "next!\n",
      "Status code 200\n",
      "last: t3_thd3xt\n",
      "obtained 100 posts\n",
      "total: 400 posts\n",
      "next!\n",
      "Status code 200\n",
      "last: t3_tgpdey\n",
      "obtained 100 posts\n",
      "total: 500 posts\n",
      "next!\n",
      "Status code 200\n",
      "last: t3_tg20uz\n",
      "obtained 100 posts\n",
      "total: 600 posts\n",
      "next!\n",
      "Status code 200\n",
      "last: t3_tf54bq\n",
      "obtained 100 posts\n",
      "total: 700 posts\n",
      "next!\n",
      "Status code 200\n",
      "last: t3_teax8m\n",
      "obtained 100 posts\n",
      "total: 800 posts\n",
      "next!\n",
      "Status code 200\n",
      "last: t3_tde4pl\n",
      "obtained 100 posts\n",
      "total: 900 posts\n",
      "next!\n",
      "Status code 200\n",
      "last: t3_tc4ztx\n",
      "obtained 75 posts\n",
      "total: 975 posts\n",
      "next!\n",
      "Status code 200\n",
      "done!\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 975 entries, 0 to 74\n",
      "Data columns (total 18 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   kind             975 non-null    object \n",
      " 1   subreddit        975 non-null    object \n",
      " 2   title            975 non-null    object \n",
      " 3   author           975 non-null    object \n",
      " 4   permalink        975 non-null    object \n",
      " 5   url              975 non-null    object \n",
      " 6   num_comments     975 non-null    float64\n",
      " 7   ups              975 non-null    float64\n",
      " 8   selftext         975 non-null    object \n",
      " 9   id               975 non-null    object \n",
      " 10  downs            975 non-null    float64\n",
      " 11  upvote_ratio     975 non-null    float64\n",
      " 12  score            975 non-null    float64\n",
      " 13  created          975 non-null    float64\n",
      " 14  edited           975 non-null    float64\n",
      " 15  author_fullname  975 non-null    object \n",
      " 16  subreddit_id     975 non-null    object \n",
      " 17  guessed_downs    975 non-null    int64  \n",
      "dtypes: float64(7), int64(1), object(10)\n",
      "memory usage: 144.7+ KB\n"
     ]
    }
   ],
   "source": [
    "#Old data\n",
    "save_file = \"scrapped_data.csv\"\n",
    "old_df = pd.read_csv(save_file)\n",
    "\n",
    "# Scrap new data\n",
    "headers = getHeaders()\n",
    "new_df = getAllPosts(keys_to_extract, headers, subreddit=\"argentina\")\n",
    "\n",
    "# Update data\n",
    "df = combineOnColAndUpdate(old_df, new_df, \"id\")\n",
    "\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abd97273",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(f\"reddit-argentina-v{math.floor(time.time())}.csv\",index=False)\n",
    "df.to_csv(save_file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc99fa7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(json.dumps(res.json()[\"data\"][\"children\"][6], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befec4a7",
   "metadata": {},
   "source": [
    "In selftext we have the post content, if the post has some text in it and is not just an image, video or link. <br>\n",
    "I want to check that the text is saved correctly in selftext if such a text exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bc9392b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perdonen chicos, tengan cuidado con los lugares de comida por peso. Me vino un gusano muerto de 1 cm en la comida.\n",
      "\n",
      "Created: 1648054293.0\n",
      "-----------------------\n",
      "\n",
      "Resulta que compr칠 comida ahi y me vino un gusano. No se preocupen, ya me met칤 los dedos por la garganta y estoy viendo si tengo que tomar alg칰n antiparasitario.\n",
      "S칠 que no todos los locales del estilo son as칤 necesariamente, pero tengan cuidado. La comida por lo general esta expuesta al aire y, por consiguiente, a las moscas que pueden dejar sus huevos en la comida y hacer que salgan gusanos. 쯃es pas칩 alguna vez? 쯈u칠 piensan al respecto?\n",
      "\n",
      "Pido perd칩n porque fui el que recomend칩 ir a lugares de comida por peso en otro post XD.\n",
      "\n",
      "Edit: Era el Wok Express de Rodr칤guez Pe침a entre Av Santa Fe y Arenales. Era un gusano de la verdura y al final no media 1 cm sino que medio cm.\n"
     ]
    }
   ],
   "source": [
    "id = \"tl0u39\"\n",
    "\n",
    "result = df[df[\"id\"] == id]\n",
    "print(result.iloc[0][\"title\"])\n",
    "print(\"\\nCreated:\", result.iloc[0][\"created\"])\n",
    "print(\"-----------------------\\n\")\n",
    "\n",
    "print(result.iloc[0][\"selftext\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
